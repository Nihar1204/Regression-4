{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Assignment - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Lasso regression is a type of linear regression that includes an L1 regularization term. This means that it not \n",
    "# only minimizes the residual sum of squares (RSS) like ordinary least squares (OLS) regression but also applies a \n",
    "# penalty proportional to the absolute sum of the coefficients. This results in some coefficients being shrunk to \n",
    "# exactly zero, effectively performing feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# The primary advantage of Lasso Regression in feature selection is that it automatically eliminates irrelevant \n",
    "# or less important features by setting their coefficients to exactly zero. This makes Lasso an efficient \n",
    "# method for reducing model complexity and improving interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# In Lasso Regression, the coefficients (𝛽𝑗) represent the relationship between each feature and the target variable. \n",
    "# However, due to L1 regularization, some coefficients may be shrunk to exactly zero, indicating that those features \n",
    "# are irrelevant to the model.\n",
    "\n",
    "# Steps to Interpret Lasso Coefficients:\n",
    "\n",
    "# Non-Zero Coefficients -> Important Features\n",
    "# Zero Coefficients -> Irrelevant Features\n",
    "# Magnitude of Coefficients -> Strength of Impact\n",
    "# 1: Larger absolute values indicate stronger influence on the target variable.\n",
    "# 2: If a coefficient is small but nonzero, the feature has a weak effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "# model's performance?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Lasso Regression has one main tuning parameter that significantly affects its performance and that is λ.\n",
    "# How it affects the model:\n",
    "\n",
    "# Larger λ: Increases the amount of regularization. This leads to:\n",
    "# More coefficients being shrunk towards zero, and potentially some being exactly zero (feature selection).   \n",
    "# Higher bias in the model.\n",
    "# Lower variance in the model.\n",
    "# Potentially underfitting if λ is too large.\n",
    "\n",
    "# Smaller λ: Decreases the amount of regularization. This leads to:\n",
    "# Fewer coefficients being shrunk towards zero.\n",
    "# Lower bias in the model.\n",
    "# Higher variance in the model.\n",
    "# Potentially overfitting if λ is too small.\n",
    "\n",
    "# We can get the optimum value of λ by using Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Yes, Lasso Regression can be used for non-linear regression problems, although it's fundamentally a linear model. \n",
    "\n",
    "# Non-linear transformations: The key to using Lasso for non-linear relationships is to transform the independent \n",
    "# variables in a way that captures the non-linearity. This involves creating new features from the original ones. \n",
    "# Common techniques include:\n",
    "# Polynomial features: Create features by raising the original variables to different powers (e.g. x, x^2, x^3). \n",
    "# This can capture curves and other non-linear patterns.   \n",
    "# Interaction terms: Create features by multiplying two or more original variables. This can capture how the effect \n",
    "# of one variable depends on the value of another.   \n",
    "# Basis functions: Use functions like splines, radial basis functions, or wavelets to create a set of new features \n",
    "# that represent the non-linear relationship.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Ridge Regression and Lasso Regression are both popular techniques used to regularize linear regression models, \n",
    "# primarily to prevent overfitting and improve generalization performance. While they share some similarities, \n",
    "# they also have key differences that make them suitable for different situations.\n",
    "\n",
    "# Differences:\n",
    "# Ridge Regression:\n",
    "# 1: Penalty Type: L2 regularization (sum of squared coefficients)\n",
    "# 2: Coefficient Shrinkage: Shrinks coefficients towards zero, but rarely exactly to zero.\n",
    "# 3: Feature Selection: Does not perform feature selection; keeps all predictors in the model.\n",
    "# 4: Handling Multicollinearity: Effective at handling multicollinearity (high correlation between predictors).\n",
    "# 5: Sparsity: Produces non-sparse solutions (most coefficients are non-zero).\n",
    "\n",
    "# Lasso Regression:\n",
    "# 1: Penalty Type: L1 regularization (sum of absolute values of coefficients)\n",
    "# 2: Coefficient Shrinkage: Shrinks coefficients towards zero, and can force some coefficients to be exactly zero.\n",
    "# 3: Feature Selection: Performs feature selection; can eliminate less important predictors by setting their \n",
    "# coefficients to zero.\n",
    "# 4: Can struggle with multicollinearity; may arbitrarily select one variable from a group of correlated \n",
    "# variables and discard the rest.\n",
    "# 5: Sparsity: Produces sparse solutions (many coefficients may be zero).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Yes, Lasso Regression can handle multicollinearity to some extent, but it does so in a unique way compared \n",
    "# to Ridge Regression.\n",
    "\n",
    "# Lasso applies L1 regularization, which encourages sparsity by forcing some coefficients to be exactly zero. \n",
    "# When faced with highly correlated features, Lasso tends to randomly select one feature and shrink the others to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# By applying Cross-Validayion techniques like GridSearchCV or RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
